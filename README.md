# Model-Compression-And-Acceleration

Sorted out some papers related to deep neural network model compression and acceleration for easy reference. They are mainly divided into five methods:
- [Pruning](#Pruning)
- Quantization
- Knowledge Distillation
- Matrix Decomposition
- Neural Architecture Search

## Pruning

### Weight Pruning
- 2015-NIPS-[Learning both Weights and Connections for Efficient Neural Networks](https://arxiv.org/abs/1506.02626)
- 2016-ICLR(Best Paper)-[Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding](https://arxiv.org/abs/1510.00149)
